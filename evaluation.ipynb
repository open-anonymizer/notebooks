{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/prepared/20210311_eval.pickle\", \"rb\") as fp:\n",
    "    eval_data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_from_tuplelist(actual_tuples,predicted_tuples): \n",
    "    \"\"\"Checks if lists for actual and predicted have the same tokens and returns sklearn confusion matrix for NER\"\"\"\n",
    "    \n",
    "    #check if tokens are similar btwn actual and predicted\n",
    "    tokens_actual = [elem[0] for elem in actual_tuples]\n",
    "    tokens_pred = [elem[0] for elem in predicted_tuples]\n",
    "    \n",
    "    if (tokens_actual == tokens_pred):\n",
    "        # return confusion matrix if all tokens are similar #TODO: is more flexibility required (does it returns errors to often?)\n",
    "        \n",
    "        NER_actual = [elem[1] for elem in actual_tuples]\n",
    "        NER_pred = [elem[1] for elem in predicted_tuples]\n",
    "    \n",
    "        return classification_report(NER_actual, NER_pred, labels=[\"PER\", \"LOC\", \"ORG\"])\n",
    "    \n",
    "    else:\n",
    "        raise NameError('tokens in list for actual do not match tokens in list prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other functions that might be useful\n",
    "\n",
    "def get_wrong_pred(actual, predicted, label):\n",
    "    \n",
    "    print(\"predicted, but not labelled\")\n",
    "    print(\"------------------------------\")\n",
    "\n",
    "    for p, a in zip(predicted, actual):\n",
    "        if p[1] == label and a[1] != label:\n",
    "            print((p, a))\n",
    "\n",
    "def get_miss_pred(actual, predicted, label):\n",
    "    \n",
    "    print(\"labelled, but not predicted\")\n",
    "    print(\"------------------------------\")\n",
    "\n",
    "    for p, a in zip(predicted_res, actual_res):\n",
    "        if p[1] != label and a[1] == label:\n",
    "            print((p, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load our models and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_pipeline(model_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "    nlp = pipeline(\n",
    "        \"ner\", model=model, tokenizer=tokenizer, grouped_entities=True\n",
    "    )\n",
    "    \n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_lrg = \"../models/xlm-roberta-large-finetuned-conll03-german\"\n",
    "nlp_large = setup_pipeline(model_path_lrg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_sml = \"../models/distilbert-base-german-cased-finetuned-germeval14-german\"\n",
    "nlp_small = setup_pipeline(model_path_sml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_sml_alt = \"../models/distilbert-base-german-cased-finetuned-conll03-german\"\n",
    "nlp_smalll_2 = setup_pipeline(model_path_sml_alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for preparing tokens \n",
    "\n",
    "def prepare_token_transformer(text, nlp_pipeline):\n",
    "    text_splitted = re.split(\"\\s|!|\\.|,|\\\\(|\\\\)\", text)\n",
    "    text_splitted = [{e: \"O\"} for e in text_splitted]\n",
    "\n",
    "    found_ents = nlp_pipeline(text)\n",
    "    found = []\n",
    "    for each in found_ents:\n",
    "        word = each.get(\"word\")\n",
    "        ent_group = each.get(\"entity_group\")\n",
    "        word_splitted = word.split()\n",
    "        for each_word in word_splitted:\n",
    "            found.append({each_word: ent_group})\n",
    "\n",
    "    keys_found = []\n",
    "    for each in found:\n",
    "        for k, v in each.items():\n",
    "            keys_found.append(k)\n",
    "\n",
    "    new_res = []\n",
    "    for each in text_splitted:\n",
    "        for k, v in each.items():\n",
    "            if k in keys_found:\n",
    "                for i in found:\n",
    "                    for k_f, v_f in i.items():\n",
    "                        if k == k_f:\n",
    "                            temp_add = i\n",
    "            else:\n",
    "                temp_add = each\n",
    "\n",
    "        new_res.append(temp_add)\n",
    "\n",
    "    return new_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for large model\n",
    "predicted = []\n",
    "actual = []\n",
    "\n",
    "for each in tqdm(eval_data):\n",
    "    \n",
    "    temp_act = each.get(\"with_labels\")\n",
    "    \n",
    "    text_to_anon = each.get(\"text\")\n",
    "    try:\n",
    "        temp_res = prepare_token_transformer(text_to_anon, nlp_large) \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        next\n",
    "        \n",
    "    temp_pred = []\n",
    "    for each in temp_res:\n",
    "        for k, v in each.items():\n",
    "            temp = (k, v)\n",
    "            temp_pred.append(temp)\n",
    "    temp_pred = [e for e in temp_pred if len(e[0]) > 1]\n",
    "            \n",
    "    if len(temp_act) != len(temp_pred):\n",
    "        print(\"unequal length\")\n",
    "        print(len(temp_act))\n",
    "        print(len(temp_pred))\n",
    "        print(temp_act)\n",
    "        print(temp_pred)\n",
    "        next\n",
    "    else:\n",
    "        actual.append(temp_act)\n",
    "        predicted.append(temp_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct for manual \"rules\" coded in framework for actual labels\n",
    "\n",
    "temp_actual_res = []\n",
    "for each in actual:\n",
    "    for i in each:\n",
    "        for k, v in i.items():\n",
    "            temp_tuple = (k, v)\n",
    "            temp_actual_res.append(temp_tuple)\n",
    "\n",
    "actual_res = []\n",
    "for t in temp_actual_res:\n",
    "    if t[0] == \"Hr\":\n",
    "        t = (\"Hr\", \"O\")\n",
    "    elif t[0] == \"Fr\":\n",
    "        t = (\"Fr\", \"O\")\n",
    "    elif t[0] == \"Frau\":\n",
    "        t = (\"Frau\", \"O\")\n",
    "    elif t[0] == \"Herr\":\n",
    "        t = (\"Herr\", \"O\")\n",
    "    actual_res.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct for manual \"rules\" coded in framework for predicted labels\n",
    "# and labelling errors ::fixme:: later\n",
    "\n",
    "predicted_res = []\n",
    "for each in predicted:\n",
    "    for t in each:\n",
    "        if t[0] == \"Hr\":\n",
    "            t = (\"Hr\", \"O\")\n",
    "        elif t[0] == \"Fr\":\n",
    "            t = (\"Fr\", \"O\")\n",
    "        elif t[0] == \"Frau\":\n",
    "            t = (\"Frau\", \"O\")\n",
    "        elif t[0] == \"Herr\":\n",
    "            t = (\"Herr\", \"O\")\n",
    "        elif t[0] == \"Markt\":\n",
    "            t = (\"Markt\", \"O\")\n",
    "        elif t[0] == \"XXX\":\n",
    "            t = (\"XXX\", \"O\")\n",
    "        predicted_res.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discuss whether Herr Frau etc should be filtered out here\n",
    "print(\"large xlm-roberta model\")\n",
    "print(get_metrics_from_tuplelist(actual_res, predicted_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### smaller distillbert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for small model\n",
    "predicted = []\n",
    "actual = []\n",
    "\n",
    "for each in tqdm(eval_data):\n",
    "    \n",
    "    temp_act = each.get(\"with_labels\")\n",
    "    \n",
    "    text_to_anon = each.get(\"text\")\n",
    "    try:\n",
    "        temp_res = prepare_token_transformer(text_to_anon, nlp_small) \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        next\n",
    "        \n",
    "    temp_pred = []\n",
    "    for each in temp_res:\n",
    "        for k, v in each.items():\n",
    "            temp = (k, v)\n",
    "            temp_pred.append(temp)\n",
    "    temp_pred = [e for e in temp_pred if len(e[0]) > 1]\n",
    "            \n",
    "    if len(temp_act) != len(temp_pred):\n",
    "        print(\"unequal length\")\n",
    "        print(len(temp_act))\n",
    "        print(len(temp_pred))\n",
    "        print(temp_act)\n",
    "        print(temp_pred)\n",
    "        next\n",
    "    else:\n",
    "        actual.append(temp_act)\n",
    "        predicted.append(temp_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct for manual \"rules\" coded in framework for actual labels\n",
    "\n",
    "temp_actual_res = []\n",
    "for each in actual:\n",
    "    for i in each:\n",
    "        for k, v in i.items():\n",
    "            temp_tuple = (k, v)\n",
    "            temp_actual_res.append(temp_tuple)\n",
    "\n",
    "actual_res = []\n",
    "for t in temp_actual_res:\n",
    "    if t[0] == \"Hr\":\n",
    "        t = (\"Hr\", \"O\")\n",
    "    elif t[0] == \"Fr\":\n",
    "        t = (\"Fr\", \"O\")\n",
    "    elif t[0] == \"Frau\":\n",
    "        t = (\"Frau\", \"O\")\n",
    "    elif t[0] == \"Herr\":\n",
    "        t = (\"Herr\", \"O\")\n",
    "    actual_res.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct for manual \"rules\" coded in framework for predicted labels\n",
    "# and labelling errors ::fixme:: later\n",
    "\n",
    "predicted_res = []\n",
    "for each in predicted:\n",
    "    for t in each:\n",
    "        if t[0] == \"Hr\":\n",
    "            t = (\"Hr\", \"O\")\n",
    "        elif t[0] == \"Fr\":\n",
    "            t = (\"Fr\", \"O\")\n",
    "        elif t[0] == \"Frau\":\n",
    "            t = (\"Frau\", \"O\")\n",
    "        elif t[0] == \"Herr\":\n",
    "            t = (\"Herr\", \"O\")\n",
    "        elif t[0] == \"Markt\":\n",
    "            t = (\"Markt\", \"O\")\n",
    "        elif t[0] == \"XXX\":\n",
    "            t = (\"XXX\", \"O\")\n",
    "        predicted_res.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discuss whether Herr Frau etc should be filtered out here\n",
    "print(\"small distillbert model\")\n",
    "print(get_metrics_from_tuplelist(actual_res, predicted_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_wrong_pred(actual_res, predicted_res, \"LOC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_miss_pred(actual_res, predicted_res, \"PER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## alternative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for small model\n",
    "predicted = []\n",
    "actual = []\n",
    "\n",
    "for each in tqdm(eval_data):\n",
    "    \n",
    "    temp_act = each.get(\"with_labels\")\n",
    "    \n",
    "    text_to_anon = each.get(\"text\")\n",
    "    try:\n",
    "        temp_res = prepare_token_transformer(text_to_anon, nlp_smalll_2) \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        next\n",
    "        \n",
    "    temp_pred = []\n",
    "    for each in temp_res:\n",
    "        for k, v in each.items():\n",
    "            temp = (k, v)\n",
    "            temp_pred.append(temp)\n",
    "    temp_pred = [e for e in temp_pred if len(e[0]) > 1]\n",
    "            \n",
    "    if len(temp_act) != len(temp_pred):\n",
    "        print(\"unequal length\")\n",
    "        print(len(temp_act))\n",
    "        print(len(temp_pred))\n",
    "        print(temp_act)\n",
    "        print(temp_pred)\n",
    "        next\n",
    "    else:\n",
    "        actual.append(temp_act)\n",
    "        predicted.append(temp_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct for manual \"rules\" coded in framework for actual labels\n",
    "\n",
    "temp_actual_res = []\n",
    "for each in actual:\n",
    "    for i in each:\n",
    "        for k, v in i.items():\n",
    "            temp_tuple = (k, v)\n",
    "            temp_actual_res.append(temp_tuple)\n",
    "\n",
    "actual_res = []\n",
    "for t in temp_actual_res:\n",
    "    if t[0] == \"Hr\":\n",
    "        t = (\"Hr\", \"O\")\n",
    "    elif t[0] == \"Fr\":\n",
    "        t = (\"Fr\", \"O\")\n",
    "    elif t[0] == \"Frau\":\n",
    "        t = (\"Frau\", \"O\")\n",
    "    elif t[0] == \"Herr\":\n",
    "        t = (\"Herr\", \"O\")\n",
    "    actual_res.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct for manual \"rules\" coded in framework for predicted labels\n",
    "# and labelling errors ::fixme:: later\n",
    "\n",
    "predicted_res = []\n",
    "for each in predicted:\n",
    "    for t in each:\n",
    "        if t[0] == \"Hr\":\n",
    "            t = (\"Hr\", \"O\")\n",
    "        elif t[0] == \"Fr\":\n",
    "            t = (\"Fr\", \"O\")\n",
    "        elif t[0] == \"Frau\":\n",
    "            t = (\"Frau\", \"O\")\n",
    "        elif t[0] == \"Herr\":\n",
    "            t = (\"Herr\", \"O\")\n",
    "        elif t[0] == \"Markt\":\n",
    "            t = (\"Markt\", \"O\")\n",
    "        elif t[0] == \"XXX\":\n",
    "            t = (\"XXX\", \"O\")\n",
    "        predicted_res.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discuss whether Herr Frau etc should be filtered out here\n",
    "print(\"small distillbert model conll\")\n",
    "print(get_metrics_from_tuplelist(actual_res, predicted_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_token_spacy(doc, text_splitted):\n",
    "    found = []\n",
    "    for ent in doc.ents:\n",
    "        word = ent.text\n",
    "        ent_group = ent.label_\n",
    "\n",
    "        word_splitted = word.split()\n",
    "        for each_word in word_splitted:\n",
    "            found.append({each_word: ent_group})\n",
    "\n",
    "    keys_found = []\n",
    "    for each in found:\n",
    "        for k, v in each.items():\n",
    "            keys_found.append(k)\n",
    "\n",
    "    new_res = []\n",
    "    for each in text_splitted:\n",
    "        for k, v in each.items():\n",
    "            if k in keys_found:\n",
    "                for i in found:\n",
    "                    for k_f, v_f in i.items():\n",
    "                        if k == k_f:\n",
    "                            temp_add = i\n",
    "            else:\n",
    "                temp_add = each\n",
    "\n",
    "        new_res.append(temp_add)\n",
    "\n",
    "    return new_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# might need to download model first\n",
    "# !python -m spacy download de_core_news_md "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = []\n",
    "predicted = []\n",
    "\n",
    "for each in tqdm(eval_data):\n",
    "    \n",
    "    temp_act = each.get(\"with_labels\")\n",
    "    \n",
    "    text_to_anon = each.get(\"text\")\n",
    "    text_splitted = re.split(\"\\s|!|\\.|,|\\\\(|\\\\)\", text_to_anon)\n",
    "    text_splitted = [{e: \"O\"} for e in text_splitted]\n",
    "    \n",
    "    doc = nlp(text_to_anon)\n",
    "    \n",
    "    try:\n",
    "        temp_res = prepare_token_spacy(doc, text_splitted)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        next\n",
    "    \n",
    "    temp_pred = []\n",
    "    \n",
    "    for each in temp_res:\n",
    "        for k, v in each.items():\n",
    "            \n",
    "            if not v in [\"PER\", \"LOC\", \"ORG\", \"O\"]:\n",
    "                temp = (k, \"O\")\n",
    "            else:\n",
    "                temp = (k, v)\n",
    "                \n",
    "            temp_pred.append(temp)\n",
    "    temp_pred = [e for e in temp_pred if len(e[0]) > 1]\n",
    "            \n",
    "    if len(temp_act) != len(temp_pred):\n",
    "        print(\"unequal length\")\n",
    "        print(len(temp_act))\n",
    "        print(len(temp_pred))\n",
    "        print(temp_act)\n",
    "        print(temp_pred)\n",
    "        next\n",
    "    else:\n",
    "        actual.append(temp_act)\n",
    "        predicted.append(temp_pred)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct for manual \"rules\" coded in framework for actual labels\n",
    "\n",
    "temp_actual_res = []\n",
    "for each in actual:\n",
    "    for i in each:\n",
    "        for k, v in i.items():\n",
    "            temp_tuple = (k, v)\n",
    "            temp_actual_res.append(temp_tuple)\n",
    "\n",
    "actual_res = []\n",
    "for t in temp_actual_res:\n",
    "    if t[0] == \"Hr\":\n",
    "        t = (\"Hr\", \"O\")\n",
    "    elif t[0] == \"Fr\":\n",
    "        t = (\"Fr\", \"O\")\n",
    "    elif t[0] == \"Frau\":\n",
    "        t = (\"Frau\", \"O\")\n",
    "    elif t[0] == \"Herr\":\n",
    "        t = (\"Herr\", \"O\")\n",
    "    actual_res.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct for manual \"rules\" coded in framework for predicted labels\n",
    "# and labelling errors ::fixme:: later\n",
    "\n",
    "predicted_res = []\n",
    "for each in predicted:\n",
    "    for t in each:\n",
    "        if t[0] == \"Hr\":\n",
    "            t = (\"Hr\", \"O\")\n",
    "        elif t[0] == \"Fr\":\n",
    "            t = (\"Fr\", \"O\")\n",
    "        elif t[0] == \"Frau\":\n",
    "            t = (\"Frau\", \"O\")\n",
    "        elif t[0] == \"Herr\":\n",
    "            t = (\"Herr\", \"O\")\n",
    "        elif t[0] == \"Markt\":\n",
    "            t = (\"Markt\", \"O\")\n",
    "        elif t[0] == \"XXX\":\n",
    "            t = (\"XXX\", \"O\")\n",
    "        predicted_res.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discuss whether Herr Frau etc should be filtered out here\n",
    "print(\"medium spacy model\")\n",
    "print(get_metrics_from_tuplelist(actual_res, predicted_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lexical approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_per = pd.read_csv(\"../lexical_approach/german_per.csv\")\n",
    "df_loc = pd.read_csv(\"../lexical_approach/german_loc.csv\")\n",
    "df_org = pd.read_csv(\"../lexical_approach/german_org.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_names = df_per[\"per\"].values\n",
    "lookup_locations = df_loc[\"loc\"].values\n",
    "lookup_orgs = df_org[\"loc\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text\n",
    "actual = []\n",
    "predicted = []\n",
    "\n",
    "for each in tqdm(eval_data):\n",
    "    \n",
    "    temp_act = each.get(\"with_labels\")\n",
    "    \n",
    "    text_to_anon = each.get(\"text\")\n",
    "    text_splitted = re.split(\"\\s|!|\\.|,|\\\\(|\\\\)\", text_to_anon)\n",
    "    text_splitted = [e for e in text_splitted if len(e) > 1]\n",
    "    \n",
    "    temp_pred = []\n",
    "    for each in text_splitted:\n",
    "        temp_res = []\n",
    "        if each in lookup_names:\n",
    "            temp_res.append(\"PER\")\n",
    "        if each in lookup_locations:\n",
    "            temp_res.append(\"LOC\")\n",
    "        if each in lookup_orgs:\n",
    "            temp_res.append(\"ORG\")\n",
    "        \n",
    "        if not temp_res:\n",
    "            temp = {each: \"O\"}\n",
    "        else: \n",
    "            temp = {each: random.choice(temp_res)}\n",
    "        temp_pred.append(temp)\n",
    "            \n",
    "    if len(temp_act) != len(temp_pred):\n",
    "        print(\"unequal length\")\n",
    "        print(len(temp_act))\n",
    "        print(len(temp_pred))\n",
    "        print(temp_act)\n",
    "        print(temp_pred)\n",
    "        next\n",
    "    else:\n",
    "        actual.append(temp_act)\n",
    "        predicted.append(temp_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct for manual \"rules\" coded in framework for actual labels\n",
    "\n",
    "temp_actual_res = []\n",
    "for each in actual:\n",
    "    for i in each:\n",
    "        for k, v in i.items():\n",
    "            temp_tuple = (k, v)\n",
    "            temp_actual_res.append(temp_tuple)\n",
    "\n",
    "actual_res = []\n",
    "for t in temp_actual_res:\n",
    "    if t[0] == \"Hr\":\n",
    "        t = (\"Hr\", \"O\")\n",
    "    elif t[0] == \"Fr\":\n",
    "        t = (\"Fr\", \"O\")\n",
    "    elif t[0] == \"Frau\":\n",
    "        t = (\"Frau\", \"O\")\n",
    "    elif t[0] == \"Herr\":\n",
    "        t = (\"Herr\", \"O\")\n",
    "    actual_res.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct for manual \"rules\" coded in framework for predicted labels\n",
    "# and labelling errors ::fixme:: later\n",
    "\n",
    "temp_predict_res = []\n",
    "for each in predicted:\n",
    "    for i in each:\n",
    "        for k, v in i.items():\n",
    "            temp_tuple = (k, v)\n",
    "            temp_predict_res.append(temp_tuple)\n",
    "            \n",
    "predicted_res = []\n",
    "for t in temp_predict_res:\n",
    "    if t[0] == \"Hr\":\n",
    "        t = (\"Hr\", \"O\")\n",
    "    elif t[0] == \"Fr\":\n",
    "        t = (\"Fr\", \"O\")\n",
    "    elif t[0] == \"Frau\":\n",
    "        t = (\"Frau\", \"O\")\n",
    "    elif t[0] == \"Herr\":\n",
    "        t = (\"Herr\", \"O\")\n",
    "    elif t[0] == \"Markt\":\n",
    "        t = (\"Markt\", \"O\")\n",
    "    elif t[0] == \"XXX\":\n",
    "        t = (\"XXX\", \"O\")\n",
    "    predicted_res.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discuss whether Herr Frau etc should be filtered out here\n",
    "print(\"lexical approach\")\n",
    "print(get_metrics_from_tuplelist(actual_res, predicted_res))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
